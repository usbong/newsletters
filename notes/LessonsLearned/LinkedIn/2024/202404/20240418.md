## https://www.sciencedirect.com/science/article/abs/pii/S1532046424000613; last accessed: 20240418

> Variation in monitoring: Glucose measurement in the ICU as a case study to preempt spurious correlations

## Authors

> Khushboo Teotia, Yueran Jia, Naira Link Woite, Leo Anthony Celi, João Matos, Tristan Struja

## Date

> Received 30 September 2023, Revised 29 March 2024, Accepted 12 April 2024, Available online 14 April 2024.

## SUMMARY

## Unfair AI models caused by "missingness"?

> One of the main challenges in developing fair AI models is missingness handling [6], [7], particularly in clinical scenarios, where not performing certain tests unless there is diagnostic suspicion is a common clinical practice, which is described as informative missingness [8]. This often leads to a phenomenon where an “absent test” in the EHR is implicitly treated as a “normal” result by the AI model [9]. 

## Objective

> Therefore, a nuanced analysis of disparities in the data is needed to understand the implicit assumptions of the missing values. 

## Observations

> A 2017 study found that 49 out of 107 electronic health record (EHR)-based risk prediction approaches evaluated did not mention missing data at all [10]. Machine learning algorithms have different approaches to handle missingness.. However, these methodologies could oversimplify the complex reasons behind test omission and may influence model fairness [11].

## More tests mean higher likelihood of finding abnormalities

> The likelihood of detecting an abnormal finding largely depends on how often a test is conducted. Thus, data that is not missing at random can drive spurious correlations, which are noncausal relationships between the input and the outcome which may shift in deployment [12]. 

## Question for investigation: Are biases causing more tests?

> When the reason for missing data is a result of systemic social discrimination, these biases can be embedded in subsequent AI models, perpetuating and exacerbating existing disparities [13], [14]. Especially, as frequency of measurements has gained interest in the AI building community [15]. 

## Case Study

> As a case study of a potential source of such spurious correlations in medical AI models, we picked glucose measurement frequency among patients with sepsis admitted to the Intensive Care Unit (ICU).

## What is Sepsis?

> Sepsis is a severe life-threatening systemic infection that has a significant impact on the health systems [16], [17], [18]. Up to 90% of ICU patients exhibit elevated glucose levels irrespective of pre-existing diabetes [19], [20], [21]. 

## Result: Which had higher chances of subsequent glucose readings?

> Specifically, males (incidence rate ratio (IRR) 1.06, 95% confidence interval (CI) 1.01–––1.21), patients who identify themselves as Hispanic (IRR 1.11, 95% CI 1.01–––1.21), or Black (IRR 1.06, 95% CI 1.01–––1.12), and patients being English proficient (IRR 1.08, 95% CI 1.01–––1.15) had higher chances of subsequent glucose readings.

## --

Muchas gracias a Dr. Leo Anthony CELI por compartirnos este articulo que aparecía a mis noticias aquí en LinkedIn. Ya no puedo buscarlo otra vez, pero intentaba entenderlo. Así que escribí algunas frases que he añadido a Usbong Balita. 

http://store.usbong.ph/server/usbongBalita.php/N8

## Unfair AI models caused by "missingness"?

## Objective

## Observations

## More tests mean higher likelihood of finding abnormalities

## Question for investigation: Are biases causing more tests?

## Case Study

## What is Sepsis?

## Result: Which had higher chances of subsequent glucose readings?
